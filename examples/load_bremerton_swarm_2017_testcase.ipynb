{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testcase Data Retrieval for Earthquake Swarms in the PNSN Network\n",
    "This notebook provides a worked example of data retrieval for the following earthquake swarm within the PNSN network:  \n",
    "The May 2017 earthquake swarm near Bremerton  \n",
    "        - **Note** this example uses just 2 days of the swarm: May 10-11, 2017.  \n",
    "        - Catalog searches show the swarm can broadly be defined between May 8 and November 2017  \n",
    "        - ...potentially longer - a declustering analysis would better define this.  \n",
    "![Event map of earthquakes in the seismic swarm near Bremerton, WA](data/2017-05-10_BRM/image.png)  \n",
    "\n",
    "\n",
    "- \"Earthquake swarm NE of Bremerton\" blog post by Renate Hartog on May 11, 2017  \n",
    "https://www.pnsn.org/blog/2017/05/11/earthquake-swarm-ne-of-bremerton\n",
    "\n",
    "\n",
    "### Script Author\n",
    "Nathan Stevens: ntsteven@uw.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from obspy import UTCDateTime, Stream\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.clients.fdsn.mass_downloader import CircularDomain, Restrictions, MassDownloader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# Define small helper function - I know it's somewhere in ObsPy, but this was easier than hunting it down...\n",
    "def m2deg(x):\n",
    "    \"\"\"\n",
    "    Convert distance `x` from meters to great-circle degrees on Earth\n",
    "    \"\"\"\n",
    "    return x/111139."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Client\n",
    "client = Client('IRIS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data selection\n",
    "Use the following fields to define an event and station query kwargs for use with the `obspy`\n",
    " - **starttime**: [obspy.UTCDateTime] Start time of query in UTC\n",
    " - **endtime**:      [obspy.UTCDateTime] End time of query in UTC\n",
    " - **latitude**:     [float] centroid latitude for queries in *degrees North*\n",
    " - **longitude**:    [float] centroid longitude for queries in *degrees East*\n",
    " - **maxradius**: [float] maximum radius for query in *degrees*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fields for query\n",
    "fields = ('starttime','endtime','latitude','longitude','maxradius')\n",
    "TSstr = \"2017-05-11T00:00:00\"\n",
    "TEstr = \"2017-05-11T06:00:00\"\n",
    "# TEstr = \"2017-05-12T23:59:59\"\n",
    "# Define data selection kwargs for events from IRIS catalog\n",
    "qevent_kwargs = dict(zip(fields,[UTCDateTime(TSstr),\\\n",
    "                                UTCDateTime(TEstr),\\\n",
    "                                47.5828,-122.57841,m2deg(7.5e3)]))\n",
    "# Define data selection kwargs for stations in IRIS catalog\n",
    "qstation_kwargs = dict(zip(fields,[UTCDateTime(TSstr),\\\n",
    "                                   UTCDateTime(TEstr),\\\n",
    "                                47.5828,-122.57841,m2deg(20e3)]))\n",
    "\n",
    "# Limit channels to those coming from broadbands ?H? and accelerometers ?N?\n",
    "CLIST = 'HHZ,HHN,HH1,HHE,HH2,ENZ,EN1,ENN,EN2,ENE,BHZ,BHN,BHZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory created at 2023-10-06T15:54:50.873500Z\n",
      "\tCreated by: IRIS WEB SERVICE: fdsnws-station | version: 1.1.52\n",
      "\t\t    http://service.iris.edu/fdsnws/station/1/query?starttime=2017-05-...\n",
      "\tSending institution: IRIS-DMC (IRIS-DMC)\n",
      "\tContains:\n",
      "\t\tNetworks (1):\n",
      "\t\t\tUW\n",
      "\t\tStations (19):\n",
      "\t\t\tUW.BABE (Bainbridge, WA, USA)\n",
      "\t\t\tUW.GMW (Gold Mountain, WA, USA)\n",
      "\t\t\tUW.GNW (Green Mountain, WA, USA)\n",
      "\t\t\tUW.GTWN (Georgetown Playfield ANSS-SMO)\n",
      "\t\t\tUW.HART (Harbor Island, Seattle, WA, USA)\n",
      "\t\t\tUW.HOLY (Holy Rosary ANSS-SMO)\n",
      "\t\t\tUW.KCAM (King County Airport Maintenance Shop, Seattle, WA, USA)\n",
      "\t\t\tUW.KDK (Kingdome, Seattle, WA, USA)\n",
      "\t\t\tUW.KIMR (Kitsap County HHW Collection Facility, Bremerton, WA, USA)\n",
      "\t\t\tUW.KINR (North Road Shop, Poulsbo, WA, USA)\n",
      "\t\t\tUW.KITP (Kitsap Treatment Plant, WA, USA)\n",
      "\t\t\tUW.LAWT (Lawton, Seattle, WA, USA)\n",
      "\t\t\tUW.MNWA (Manchester, WA)\n",
      "\t\t\tUW.PSNS (Puget Sound Naval Shipyard - ANSS SMO)\n",
      "\t\t\tUW.RAD1 (RAD site 1, Seattle, WA)\n",
      "\t\t\tUW.RAD4 (RAD site 4, Seattle, WA)\n",
      "\t\t\tUW.RAD6 (RAD site 6, Seattle, WA)\n",
      "\t\t\tUW.SSS2 (John Stanford Center, Seattle, WA, USA)\n",
      "\t\t\tUW.VVHS (Vashon HS ANSS-SMO)\n",
      "\t\tChannels (59):\n",
      "\t\t\tUW.BABE..ENZ, UW.BABE..ENN, UW.BABE..ENE, UW.GMW..ENZ, UW.GMW..ENN\n",
      "\t\t\tUW.GMW..ENE, UW.GNW..BHZ, UW.GNW..BHN, UW.GNW..ENZ, UW.GNW..ENN, \n",
      "\t\t\tUW.GNW..ENE, UW.GTWN..ENZ, UW.GTWN..ENN, UW.GTWN..ENE, UW.HART..ENZ\n",
      "\t\t\tUW.HART..ENN, UW.HART..ENE, UW.HOLY..ENZ, UW.HOLY..ENN, \n",
      "\t\t\tUW.HOLY..ENE, UW.KCAM..ENZ, UW.KCAM..ENN, UW.KCAM..ENE, UW.KDK..ENZ\n",
      "\t\t\tUW.KDK..ENN, UW.KDK..ENE, UW.KIMR..ENZ, UW.KIMR..ENN, \n",
      "\t\t\tUW.KIMR..ENE, UW.KINR..ENZ, UW.KINR..ENN, UW.KINR..ENE, \n",
      "\t\t\tUW.KITP..ENZ, UW.KITP..ENN, UW.KITP..ENE, UW.LAWT..ENZ, \n",
      "\t\t\tUW.LAWT..ENN, UW.LAWT..ENE, UW.MNWA..ENZ, UW.MNWA..ENN, \n",
      "\t\t\tUW.MNWA..ENE, UW.PSNS..ENZ, UW.PSNS..ENN, UW.PSNS..ENE, \n",
      "\t\t\tUW.RAD1..HHZ, UW.RAD1..HHN, UW.RAD1..HHE, UW.RAD4..HHZ, \n",
      "\t\t\tUW.RAD4..HHN, UW.RAD4..HHE, UW.RAD6..HHZ, UW.RAD6..HHN, \n",
      "\t\t\tUW.RAD6..HHE, UW.SSS2..ENZ, UW.SSS2..ENN, UW.SSS2..ENE, \n",
      "\t\t\tUW.VVHS..ENZ, UW.VVHS..ENN, UW.VVHS..ENE\n"
     ]
    }
   ],
   "source": [
    "# Download inventory with specified query down to channel \n",
    "# (ML generally operates on data w/o instrument response correction)\n",
    "inv = client.get_stations(**qstation_kwargs,level='channel',channel=CLIST)#,channel='??[ZNE12]')\n",
    "print(inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use IRIS webservices / FDSN to query waveform data \n",
    "# # Based on mass_downloader example on docs.github.org: https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.mass_downloader.html\n",
    "# domain = CircularDomain(**dict((k_,qstation_kwargs[k_]) for k_ in ['latitude','longitude','maxradius']),minradius=.0)\n",
    "\n",
    "# restrictions = Restrictions(starttime=UTCDateTime(TSstr),endtime=UTCDateTime(TEstr),\n",
    "#                             chunklength_in_sec=3600.)\n",
    "#                             # limit_stations_to_inventory=inv,chunklength_in_sec=3600)\n",
    "# mdl = MassDownloader()\n",
    "# mdl.download(domain,restrictions,threads_per_client=2,\\\n",
    "#              mseed_storage=os.path.join('data','2017-05_BRM'),\\\n",
    "#              stationxml_storage=os.path.join('data','2017-05_BRM','{network}','{station}.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Station' object has no attribute 'available_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/nates/Code/PNSN/Sci-Prod-ML/examples/load_bremerton_swarm_2017_testcase.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nates/Code/PNSN/Sci-Prod-ML/examples/load_bremerton_swarm_2017_testcase.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inv\u001b[39m.\u001b[39;49mnetworks[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mstations[\u001b[39m10\u001b[39;49m]\u001b[39m.\u001b[39;49mavailable_channels\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Station' object has no attribute 'available_channels'"
     ]
    }
   ],
   "source": [
    "inv.networks[0].stations[10].available_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running window 2017-05-11T00:00:00 -- 2017-05-11T01:00:00\n",
      "Running window 2017-05-11T01:00:00 -- 2017-05-11T02:00:00\n",
      "Running window 2017-05-11T02:00:00 -- 2017-05-11T03:00:00\n",
      "Running window 2017-05-11T03:00:00 -- 2017-05-11T04:00:00\n",
      "Running window 2017-05-11T04:00:00 -- 2017-05-11T05:00:00\n",
      "Running window 2017-05-11T05:00:00 -- 2017-05-11T06:00:00\n"
     ]
    }
   ],
   "source": [
    "# Get continuous waveforms using BULK request - this successfully pulls all data\n",
    "ts = UTCDateTime(TSstr)\n",
    "te = ts + 3600.\n",
    "out_file_FMT = os.path.join('data','2017-05_BRM','{NET}','{STA}','{NET}.{STA}.{CHANS}.{TS}.mseed')\n",
    "bulk = []         \n",
    "while ts < UTCDateTime(TEstr):\n",
    "    print('Running window %s -- %s'%(ts.isoformat(),te.isoformat()))\n",
    "    # Compose bulk request\n",
    "    bulk = []\n",
    "    for N_ in inv.networks:\n",
    "        for S_ in N_.stations:\n",
    "            try:\n",
    "                os.makedirs(os.path.join('data','2017-05_BRM',N_.code,S_.code))\n",
    "            except:\n",
    "                pass\n",
    "            for C_ in S_.channels:\n",
    "                CC_ = C_.code\n",
    "                LC_ = C_.location_code\n",
    "                bulk.append((N_.code,S_.code,LC_,CC_,ts,te))\n",
    "    # Execute bulk request\n",
    "    st = client.get_waveforms_bulk(bulk)\n",
    "    # Save waveforms in ML-compliant ordering - MSEED @ station level with Z,[N1],[E2] channel sequencing\n",
    "    for N_ in inv.networks:\n",
    "        for S_ in N_.stations:\n",
    "            sst = st.copy().select(network=N_.code,station=S_.code)\n",
    "            nchans = len(sst)\n",
    "\n",
    "            # If 3-C station, bundle data as normal\n",
    "            if nchans == 3:\n",
    "                ist = Stream()\n",
    "                ist += sst.select(channel='??Z')\n",
    "                ist += sst.select(channel='??[N1]')\n",
    "                ist += sst.select(channel='??[E1]')            \n",
    "                CNAMES = ist[0].stats.channel + ist[1].stats.channel[-1] + ist[2].stats.channel[-1]\n",
    "                outname = out_file_FMT.format(NET=N_.code,STA=S_.code,CHANS=CNAMES,TS=ts.isoformat())\n",
    "                ist.write(outname,fmt='MSEED')\n",
    "            # If 1-C station, use the \"trick\" from Retailleau et al. (2022)\n",
    "            elif nchans == 1:\n",
    "                ist = Stream()\n",
    "                ist += sst[0]\n",
    "                tr1f = sst[0].copy()\n",
    "                tr1f.stats.channel+= 'N'  \n",
    "                ist += tr1f\n",
    "                tr2f = sst[0].copy()\n",
    "                tr2f.stats.channel+= 'E'\n",
    "                ist += tr2f\n",
    "                CNAMES = sst[0].stats.channel + 'ZZ'\n",
    "                outname = out_file_FMT.format(NET=N_.code,STA=S_.code,CHANS=CNAMES,TS=ts.isoformat())\n",
    "                ist.write(outname,fmt='MSEED')\n",
    "            # Deal with multi-sensor/-sampling_rate stations\n",
    "            elif nchans > 3 and nchans%3 == 0:\n",
    "                CPlist = []\n",
    "                for C_ in S_.channels:\n",
    "                    if C_.code[:2] not in CPlist:\n",
    "                        CPlist.append(C_.code[:2])\n",
    "                for CP_ in CPlist:\n",
    "                    ist = Stream()\n",
    "                    ist += sst.select(channel=CP_+'Z')\n",
    "                    ist += sst.select(channel=CP_+'[N1]')\n",
    "                    ist += sst.select(channel=CP_+'[E1]')            \n",
    "                    CNAMES = ist[0].stats.channel + ist[1].stats.channel[-1] + ist[2].stats.channel[-1]\n",
    "                    outname = out_file_FMT.format(NET=N_.code,STA=S_.code,CHANS=CNAMES,TS=ts.isoformat())\n",
    "                    ist.write(outname,fmt='MSEED')\n",
    "            \n",
    "    # Advance indices for WHILE loop\n",
    "    ts += 3600.\n",
    "    te += 3600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Event(s) in Catalog:\n",
      "2017-05-11T00:41:03.700000Z | +47.598, -122.546 | 0.98 Ml\n"
     ]
    }
   ],
   "source": [
    "# Download catalog from Client\n",
    "cat = client.get_events(**qevent_kwargs)\n",
    "print(cat.__str__(print_all=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "                         Evid  Magnitude Magnitude Type    Epoch(UTC)  \\\n",
      "Time UTC                                                                \n",
      "2017-05-11 00:41:03  61264902        1.0              l  1.494463e+09   \n",
      "2017-05-11 01:49:33  61264912        0.3              d  1.494467e+09   \n",
      "2017-05-11 02:31:17  61264922        1.5              l  1.494470e+09   \n",
      "2017-05-11 07:35:24  61265017        3.5              l  1.494488e+09   \n",
      "2017-05-11 07:40:44  61265022        1.0              l  1.494488e+09   \n",
      "2017-05-11 07:41:13  60134708        0.3              d  1.494488e+09   \n",
      "2017-05-11 08:19:37  61265037        1.3              l  1.494491e+09   \n",
      "2017-05-11 08:37:57  61265047        1.5              l  1.494492e+09   \n",
      "2017-05-11 09:00:56  61265062        1.1              l  1.494493e+09   \n",
      "2017-05-11 09:04:43  61265067        0.8              l  1.494493e+09   \n",
      "2017-05-11 09:19:09  61265072        2.1              l  1.494494e+09   \n",
      "2017-05-11 09:27:40  61265077        1.1              l  1.494495e+09   \n",
      "2017-05-11 09:36:36  61265082        2.6              l  1.494495e+09   \n",
      "2017-05-11 10:27:18  61265092        1.3              l  1.494498e+09   \n",
      "2017-05-11 13:57:45  61265172        0.3              d  1.494511e+09   \n",
      "2017-05-11 22:02:48  61265307        1.1              l  1.494540e+09   \n",
      "2017-05-11 22:04:52  61265312        1.5              l  1.494540e+09   \n",
      "2017-05-11 22:05:13  60134908        1.6              l  1.494540e+09   \n",
      "\n",
      "                                  Time Local              Distance From  \\\n",
      "Time UTC                                                                  \n",
      "2017-05-11 00:41:03  2017/05/10 17:41:03 PDT  7.2 km from Bremerton, WA   \n",
      "2017-05-11 01:49:33  2017/05/10 18:49:33 PDT  7.9 km from Bremerton, WA   \n",
      "2017-05-11 02:31:17  2017/05/10 19:31:17 PDT  5.4 km from Bremerton, WA   \n",
      "2017-05-11 07:35:24  2017/05/11 00:35:24 PDT  4.2 km from Bremerton, WA   \n",
      "2017-05-11 07:40:44  2017/05/11 00:40:44 PDT  9.6 km from Bremerton, WA   \n",
      "2017-05-11 07:41:13  2017/05/11 00:41:13 PDT  7.3 km from Bremerton, WA   \n",
      "2017-05-11 08:19:37  2017/05/11 01:19:37 PDT  5.9 km from Bremerton, WA   \n",
      "2017-05-11 08:37:57  2017/05/11 01:37:57 PDT  5.3 km from Bremerton, WA   \n",
      "2017-05-11 09:00:56  2017/05/11 02:00:56 PDT  7.7 km from Bremerton, WA   \n",
      "2017-05-11 09:04:43  2017/05/11 02:04:43 PDT  3.8 km from Bremerton, WA   \n",
      "2017-05-11 09:19:09  2017/05/11 02:19:09 PDT  5.1 km from Bremerton, WA   \n",
      "2017-05-11 09:27:40  2017/05/11 02:27:40 PDT  6.7 km from Bremerton, WA   \n",
      "2017-05-11 09:36:36  2017/05/11 02:36:36 PDT  5.5 km from Bremerton, WA   \n",
      "2017-05-11 10:27:18  2017/05/11 03:27:18 PDT  5.9 km from Bremerton, WA   \n",
      "2017-05-11 13:57:45  2017/05/11 06:57:45 PDT  8.8 km from Bremerton, WA   \n",
      "2017-05-11 22:02:48  2017/05/11 15:02:48 PDT  5.6 km from Bremerton, WA   \n",
      "2017-05-11 22:04:52  2017/05/11 15:04:52 PDT  9.6 km from Bremerton, WA   \n",
      "2017-05-11 22:05:13  2017/05/11 15:05:13 PDT  7.4 km from Bremerton, WA   \n",
      "\n",
      "                         Lat       Lon  Depth Km  Depth Mi  \n",
      "Time UTC                                                    \n",
      "2017-05-11 00:41:03  47.5977 -122.5462      19.1      11.8  \n",
      "2017-05-11 01:49:33  47.6067 -122.5437      18.2      11.3  \n",
      "2017-05-11 02:31:17  47.5963 -122.5733      23.3      14.5  \n",
      "2017-05-11 07:35:24  47.5887 -122.5842      24.1      14.9  \n",
      "2017-05-11 07:40:44  47.6168 -122.5262      16.3      10.1  \n",
      "2017-05-11 07:41:13  47.5668 -122.5332      19.6      12.2  \n",
      "2017-05-11 08:19:37  47.5980 -122.5662      20.0      12.4  \n",
      "2017-05-11 08:37:57  47.5952 -122.5733      21.2      13.2  \n",
      "2017-05-11 09:00:56  47.6083 -122.5477      18.8      11.6  \n",
      "2017-05-11 09:04:43  47.5907 -122.5940      21.8      13.5  \n",
      "2017-05-11 09:19:09  47.5958 -122.5778      21.6      13.4  \n",
      "2017-05-11 09:27:40  47.6000 -122.5552      21.8      13.5  \n",
      "2017-05-11 09:36:36  47.5897 -122.5647      21.6      13.4  \n",
      "2017-05-11 10:27:18  47.5940 -122.5627      21.3      13.2  \n",
      "2017-05-11 13:57:45  47.5923 -122.5200      11.3       7.0  \n",
      "2017-05-11 22:02:48  47.5557 -122.5568      21.1      13.1  \n",
      "2017-05-11 22:04:52  47.6202 -122.5305      17.4      10.8  \n",
      "2017-05-11 22:05:13  47.6047 -122.5487      18.7      11.6  \n"
     ]
    }
   ],
   "source": [
    "# Load PNSN queried catalog data from custom queries for Bremerton-Adjacent Swarm (BRM)\n",
    "df_BRM_full = pd.read_csv(os.path.join('data','2017-05_BRM','pnsn_event_catalog_2017_BRM_cluster.csv'),\\\n",
    "                          parse_dates=['Time UTC'],index_col='Time UTC')\n",
    "# Sort by index\n",
    "df_BRM_full = df_BRM_full.sort_index()\n",
    "# Filter by query times\n",
    "# Translate from obspy.UTCDateTime --> pandas.Timestamp\n",
    "pdTS = pd.Timestamp(TSstr)\n",
    "pdTE = pd.Timestamp(TEstr)\n",
    "\n",
    "# Filter to period of interest\n",
    "df_EVE = df_BRM_full[(df_BRM_full.index >= pdTS) & (df_BRM_full.index <= pdTE)]\n",
    "print(len(df_EVE))\n",
    "print(df_EVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use inventory to pull data for the period of interest and save to disk in 1 hour chunks\n",
    "# ts = UTCDateTime(TSstr)\n",
    "# te = ts + 3600.\n",
    "# save_dir = os.path.join('data','2017-05_BRM')\n",
    "# while ts < UTCDateTime(TEstr):\n",
    "#     st = client.get_waveforms(starttime=ts,endtime=te)\n",
    "#     ts += 3600.\n",
    "#     te += 3600.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running window 2017-05-11T00:00:00 -- 2017-05-11T01:00:00\n",
      "Running window 2017-05-11T01:00:00 -- 2017-05-11T02:00:00\n",
      "Running window 2017-05-11T02:00:00 -- 2017-05-11T03:00:00\n"
     ]
    }
   ],
   "source": [
    "# # Get continuous waveforms - OBSOLITED - This method fails to get extensive data\n",
    "# ts = UTCDateTime(TSstr)\n",
    "# te = ts + 3600.\n",
    "# out_file_FMT = os.path.join('data','2017-05_BRM','{NET}','{STA}','{NET}.{STA}.{LOC}.{CHAN}.{TS}.mseed')\n",
    "             \n",
    "# while ts < UTCDateTime(TEstr):\n",
    "#     print('Running window %s -- %s'%(ts.isoformat(),te.isoformat()))\n",
    "#     for N_ in inv.networks:\n",
    "#         # try:\n",
    "#         #     os.makedirs(os.path.join('data','2017-05_BRM',str(N_.code)))\n",
    "#         # except:\n",
    "#         #     pass\n",
    "#         for S_ in N_.stations:\n",
    "#             st = Stream()\n",
    "#             # try:\n",
    "#             #     os.mkdir(os.path.join('data','2017-05_BRM',str(N_.code),str(S_.code)))\n",
    "#             # except:\n",
    "#             #     pass\n",
    "#             for C_ in S_.channels:\n",
    "#                 CC_ = C_.code\n",
    "#                 LC_ = C_.location_code\n",
    "#                 out_file = out_file_FMT.format(NET=N_.code,STA=S_.code,LOC=LC_,CHAN=CC_,TS=ts.isoformat())\n",
    "#                 try:\n",
    "#                     tr = client.get_waveforms(N_.code,S_.code,LC_,CC_,ts,te)[0]\n",
    "#                     try: \n",
    "#                         os.mkdirs(os.path.join('data','2017-05_BRM',str(N_.code),str(S_.code)))\n",
    "#                     except:\n",
    "#                         pass\n",
    "#                     tr.write(out_file,fmt='MSEED')\n",
    "#                 except:\n",
    "#                     pass\n",
    "#     ts += 3600.\n",
    "#     te += 3600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the catalog\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "# axa = fig.add_subplot(211)\n",
    "# axa.plot(df_BRM_full['Magnitude'],'.')\n",
    "# axa.set_xlabel('UTC Date Time')\n",
    "# axa.set_ylabel('Magnitude (mixed methods)')\n",
    "# axb = axa.twinx()\n",
    "# axb.plot(df_BRM_full['Magnitude'].cumsum(),'r-',alpha=0.5)\n",
    "# axb.set_ylabel('Cumulative magnitude',labelpad=15)\n",
    "\n",
    "# # # Define window to assess\n",
    "# # TS = pd.Timestamp(\"2017-05-11\")\n",
    "# # TE = pd.Timestamp(\"2017-05-13T00:00:00\")\n",
    "\n",
    "# axa.fill_between([TS,TE],[axa.get_ylim()[0]]*2,[axa.get_ylim()[1]]*2,color='yellow')\n",
    "\n",
    "\n",
    "# axc = fig.add_subplot(212)\n",
    "# IND = (df_BRM_full.index >= TS) & (df_BRM_full.index <= TE)\n",
    "# axc.plot(df_BRM_full[IND]['Magnitude'],'.')\n",
    "# axd = axc.twinx()\n",
    "# axd.plot((10**df_BRM_full[IND]['Magnitude']).cumsum(),'r-',alpha=0.5)\n",
    "\n",
    "# # axc = fig.add_subplot(223)\n",
    "# # ch = axc.scatter(df_BRM_full['Lon'],df_BRM_full['Lat'],s = 5**df_BRM_full['Magnitude'],c=df_BRM_full.index)\n",
    "# # plt.colorbar(ax=axc,cax=ch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
